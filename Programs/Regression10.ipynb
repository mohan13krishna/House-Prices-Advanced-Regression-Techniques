import pandas as pd
import numpy as np
from sklearn.model_selection import KFold, cross_val_score
from sklearn.linear_model import ElasticNet, Lasso, Ridge
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import RobustScaler
from sklearn.ensemble import StackingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

# Load data
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')
test_ID = test['Id']

# Load best previous submission for blending
try:
    prev_submission = pd.read_csv('new_submission.csv')  # Your 0.11868 submission
except FileNotFoundError:
    print("Error: new_submission.csv not found. Please ensure the file is in the working directory.")
    raise

# Check for unseen Neighborhoods in test set
unseen_neighs = set(test['Neighborhood']) - set(train['Neighborhood'])
print(f"Unseen Neighborhoods in test: {unseen_neighs}")

# Outlier removal (inspired by top solution)
tem_train = train.copy()
tem_train['SalePrice_log'] = np.log(train['SalePrice'])
tem_train['above_and_ground_area'] = train['TotalBsmtSF'] + train['GrLivArea']
tem_train['one_and_two'] = train['1stFlrSF'] + train['2ndFlrSF']

logi_0 = (tem_train['SalePrice_log'] >= 12.25) & (tem_train['OverallQual'] == 4)
logi_1 = (tem_train['SalePrice_log'] <= 11.5) & (tem_train['OverallQual'] == 7)
logi_2 = (tem_train['SalePrice_log'] <= 12.5) & (tem_train['OverallQual'] == 10)
logi_3 = (tem_train['SalePrice_log'] <= 12.5) & (tem_train['above_and_ground_area'] >= 6000)
logi_4 = (tem_train['SalePrice_log'] <= 11) & (tem_train['ExterQual'] == "Gd")
logi_5 = (tem_train['SalePrice_log'] <= 12.5) & (tem_train['one_and_two'] >= 4000)
logi_6 = (tem_train['SalePrice_log'] <= 11.5) & (tem_train['GarageArea'] >= 1200)
logi_7 = (tem_train['SalePrice_log'] <= 12.5) & (tem_train['TotalBsmtSF'] >= 5000)

logi = logi_0 | logi_1 | logi_2 | logi_3 | logi_4 | logi_5 | logi_6 | logi_7
train = train[~logi]

# Original outlier removal
train = train[~((train['GrLivArea'] > 4000) & (train['SalePrice'] < 300000))]

# All data
all_data = pd.concat((train.drop(['Id', 'SalePrice'], axis=1), test.drop('Id', axis=1))).reset_index(drop=True)

# Advanced features
def area_per_car(row):
    if row['GarageCars'] != 0:
        return row['GarageArea'] / row['GarageCars']
    else:
        return 0

all_data["area_per_car"] = all_data.apply(area_per_car, axis=1)
all_data["above_and_ground_area"] = all_data["TotalBsmtSF"].fillna(0) + all_data["GrLivArea"].fillna(0)
all_data["one_and_two"] = all_data["1stFlrSF"].fillna(0) + all_data["2ndFlrSF"].fillna(0)
all_data['Total_Bathrooms'] = all_data['FullBath'].fillna(0) + 0.5 * all_data['HalfBath'].fillna(0) + all_data['BsmtFullBath'].fillna(0) + 0.5 * all_data['BsmtHalfBath'].fillna(0)
all_data['Total_Porch_Area'] = all_data['OpenPorchSF'].fillna(0) + all_data['3SsnPorch'].fillna(0) + all_data['EnclosedPorch'].fillna(0) + all_data['ScreenPorch'].fillna(0) + all_data['WoodDeckSF'].fillna(0)
all_data['TotalSF'] = all_data['TotalBsmtSF'].fillna(0) + all_data['1stFlrSF'].fillna(0) + all_data['2ndFlrSF'].fillna(0)
all_data['HouseAge'] = all_data['YrSold'] - all_data['YearBuilt']
all_data['RemodAge'] = all_data['YrSold'] - all_data['YearRemodAdd']
all_data['Qual_SF'] = all_data['OverallQual'] * all_data['TotalSF']
all_data['Qual_GrLiv'] = all_data['OverallQual'] * all_data['GrLivArea']

# Handle missing values
all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))
all_data['GarageArea'] = all_data.groupby('Neighborhood')['GarageArea'].transform(lambda x: x.fillna(x.median()))
all_data['TotalBsmtSF'] = all_data.groupby('Neighborhood')['TotalBsmtSF'].transform(lambda x: x.fillna(x.median()))
numeric_feats = all_data.dtypes[all_data.dtypes != "object"].index
all_data[numeric_feats] = all_data[numeric_feats].fillna(0)
categorical_feats = all_data.dtypes[all_data.dtypes == "object"].index
all_data[categorical_feats] = all_data[categorical_feats].fillna('None')

# Log-transform skewed features
skewed_feats = all_data[numeric_feats].apply(lambda x: x.skew()).sort_values(ascending=False)
skewness = skewed_feats[skewed_feats > 0.75].index
all_data[skewness] = np.log1p(all_data[skewness])

# Target encoding for Neighborhood with smoothing
train_neigh_means = train.groupby('Neighborhood')['SalePrice'].mean()
global_mean = train['SalePrice'].mean()
alpha = 20  # Strong smoothing
neigh_counts = train['Neighborhood'].value_counts()
all_data['Neighborhood_Encoded'] = all_data['Neighborhood'].map(
    lambda x: (train_neigh_means.get(x, global_mean) * neigh_counts.get(x, 0) + global_mean * alpha) /
              (neigh_counts.get(x, 0) + alpha)
)

# One-hot encoding
all_data = pd.get_dummies(all_data.drop('Neighborhood', axis=1))
all_data = all_data.fillna(all_data.mean())

# Feature selection based on XGBoost importance
model_xgb_temp = XGBRegressor(n_estimators=200, learning_rate=0.05, max_depth=3, random_state=7, n_jobs=-1)
model_xgb_temp.fit(all_data[:train.shape[0]], np.log1p(train['SalePrice']))
importances = pd.Series(model_xgb_temp.feature_importances_, index=all_data.columns)
top_features = importances.nlargest(80).index  # Select top 80 features
all_data = all_data[top_features]

# Split
ntrain = train.shape[0]
X = all_data[:ntrain]
X_test = all_data[ntrain:]
y = np.log1p(train['SalePrice'])

# Models (inspired by your 0.11868 submission)
lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state=1))
ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=0.9, random_state=3))
GBoost = GradientBoostingRegressor(n_estimators=2000, learning_rate=0.05, max_depth=4, max_features='sqrt',
                                   min_samples_leaf=15, min_samples_split=10, loss='huber', random_state=5)
model_xgb = XGBRegressor(colsample_bytree=0.5, learning_rate=0.05, max_depth=3,
                         n_estimators=1000, reg_alpha=0.5, reg_lambda=1.0,
                         subsample=0.6, verbosity=0, random_state=7, n_jobs=-1)
model_lgb = LGBMRegressor(objective='regression', num_leaves=5, learning_rate=0.05, n_estimators=600,
                          max_bin=55, bagging_fraction=0.8, bagging_freq=5, feature_fraction=0.25,
                          min_data_in_leaf=6, min_sum_hessian_in_leaf=10, random_state=42)

# Stacking
estimators = [
    ('lasso', lasso),
    ('enet', ENet),
    ('gboost', GBoost),
    ('xgb', model_xgb),
    ('lgb', model_lgb)
]
stack = StackingRegressor(estimators=estimators, final_estimator=Ridge(alpha=1.0), cv=5, n_jobs=-1)

# Local CV (fixed RMSLE calculation)
def rmsle_cv(model):
    kf = KFold(5, shuffle=True, random_state=42)
    rmse = np.sqrt(-cross_val_score(model, X, y, scoring="neg_mean_squared_error", cv=kf))
    return rmse

score = rmsle_cv(stack)
print(f"Stacking CV score: {score.mean():.5f} (std: {score.std():.5f})")

# Fit and predict
stack.fit(X, y)
preds = np.expm1(stack.predict(X_test))

# Blend with best previous submission (0.1 new + 0.9 prev)
blended_preds = 0.1 * preds + 0.9 * prev_submission['SalePrice'].values

# Create submission
submission = pd.DataFrame({"Id": test_ID, "SalePrice": blended_preds})
submission.to_csv("improved_submission.csv", index=False)
print(f"Improved submission created at {pd.Timestamp.now('Asia/Kolkata').strftime('%I:%M %p IST, %A, %B %d, %Y')}! Submit to Kaggle to check your score.")
