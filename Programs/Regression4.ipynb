{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e220ff48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unseen Neighborhoods in test: set()\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Stacking CV score: 0.11258 (std: 0.00904)\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=10, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Triple-blended submission created! Submit to Kaggle to check your score.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "test_ID = test['Id']\n",
    "\n",
    "# Load previous submissions for blending\n",
    "prev_submission_1 = pd.read_csv('new_submission.csv')  # Your 0.11868 submission\n",
    "prev_submission_2 = pd.read_csv('blended_submission.csv')  # Your 0.12146 submission\n",
    "\n",
    "# Check for unseen Neighborhoods in test set\n",
    "unseen_neighs = set(test['Neighborhood']) - set(train['Neighborhood'])\n",
    "print(f\"Unseen Neighborhoods in test: {unseen_neighs}\")\n",
    "\n",
    "# Remove outliers (GrLivArea > 4000 and SalePrice < 300000)\n",
    "train = train[~((train['GrLivArea'] > 4000) & (train['SalePrice'] < 300000))]\n",
    "\n",
    "# Feature engineering\n",
    "all_data = pd.concat((train.drop(['Id', 'SalePrice'], axis=1), test.drop('Id', axis=1))).reset_index(drop=True)\n",
    "\n",
    "# Ensure numeric columns\n",
    "all_data['YrSold'] = all_data['YrSold'].astype(float)\n",
    "all_data['YearBuilt'] = all_data['YearBuilt'].astype(float)\n",
    "all_data['YearRemodAdd'] = all_data['YearRemodAdd'].astype(float)\n",
    "\n",
    "# Advanced features\n",
    "all_data['TotalSF'] = all_data['TotalBsmtSF'].fillna(0) + all_data['1stFlrSF'].fillna(0) + all_data['2ndFlrSF'].fillna(0)\n",
    "all_data['HouseAge'] = all_data['YrSold'] - all_data['YearBuilt']\n",
    "all_data['RemodAge'] = all_data['YrSold'] - all_data['YearRemodAdd']\n",
    "all_data['QualCond'] = all_data['OverallQual'] * all_data['OverallCond']\n",
    "all_data['TotalBath'] = all_data['FullBath'].fillna(0) + all_data['BsmtFullBath'].fillna(0) + 0.5 * (all_data['HalfBath'].fillna(0) + all_data['BsmtHalfBath'].fillna(0))\n",
    "all_data['TotalPorchSF'] = all_data['OpenPorchSF'].fillna(0) + all_data['EnclosedPorch'].fillna(0) + all_data['3SsnPorch'].fillna(0) + all_data['ScreenPorch'].fillna(0)\n",
    "all_data['Qual_SF'] = all_data['OverallQual'] * all_data['TotalSF']\n",
    "\n",
    "# Handle missing values\n",
    "all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "all_data[numeric_feats] = all_data[numeric_feats].fillna(0)\n",
    "categorical_feats = all_data.dtypes[all_data.dtypes == \"object\"].index\n",
    "all_data[categorical_feats] = all_data[categorical_feats].fillna('None')\n",
    "\n",
    "# Clip extreme values to match train distribution\n",
    "ntrain = train.shape[0]\n",
    "for col in ['LotArea', 'GrLivArea', 'TotalSF']:\n",
    "    if col in train.columns:\n",
    "        upper_limit = train[col].quantile(0.99)  # Use train for original columns\n",
    "    else:\n",
    "        upper_limit = all_data[:ntrain][col].quantile(0.99)  # Use all_data[:ntrain] for derived columns\n",
    "    all_data[col] = all_data[col].clip(upper=upper_limit)\n",
    "\n",
    "# Log-transform skewed features\n",
    "skewed_feats = ['LotArea', 'GrLivArea', 'TotalSF', 'TotalPorchSF']\n",
    "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "\n",
    "# Frequency encoding for Neighborhood\n",
    "neigh_freq = all_data['Neighborhood'].value_counts().to_dict()\n",
    "all_data['Neighborhood_Encoded'] = all_data['Neighborhood'].map(neigh_freq)\n",
    "\n",
    "# One-hot encoding for categoricals\n",
    "all_data = pd.get_dummies(all_data.drop('Neighborhood', axis=1))\n",
    "all_data = all_data.fillna(all_data.mean())\n",
    "\n",
    "# Split back to train/test\n",
    "X = all_data[:ntrain]\n",
    "X_test = all_data[ntrain:]\n",
    "y = np.log1p(train['SalePrice'])\n",
    "\n",
    "# Define base models\n",
    "model_xgb = XGBRegressor(colsample_bytree=0.5, learning_rate=0.05, max_depth=3,\n",
    "                         n_estimators=800, reg_alpha=0.7, reg_lambda=1.2,\n",
    "                         subsample=0.6, verbosity=0, random_state=7, n_jobs=-1)\n",
    "model_lgb = LGBMRegressor(objective='regression', num_leaves=4, learning_rate=0.05, n_estimators=400,\n",
    "                          max_bin=50, bagging_fraction=0.8, bagging_freq=5, feature_fraction=0.3,\n",
    "                          min_data_in_leaf=5, min_sum_hessian_in_leaf=10, random_state=42)\n",
    "model_cat = CatBoostRegressor(iterations=600, learning_rate=0.05, depth=5, silent=True, random_seed=42)\n",
    "model_gboost = GradientBoostingRegressor(n_estimators=1500, learning_rate=0.05, max_depth=3, max_features='sqrt',\n",
    "                                        min_samples_leaf=10, min_samples_split=10, loss='huber', random_state=5)\n",
    "\n",
    "# Stacking setup with Ridge meta-learner\n",
    "estimators = [\n",
    "    ('xgb', model_xgb),\n",
    "    ('lgb', model_lgb),\n",
    "    ('cat', model_cat),\n",
    "    ('gboost', model_gboost)\n",
    "]\n",
    "stack = StackingRegressor(estimators=estimators, final_estimator=Ridge(alpha=1.0), cv=5, n_jobs=-1)\n",
    "\n",
    "# Local CV evaluation (RMSLE)\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(5, shuffle=True, random_state=42)\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kf))\n",
    "    return rmse\n",
    "\n",
    "score = rmsle_cv(stack)\n",
    "print(f\"Stacking CV score: {score.mean():.5f} (std: {score.std():.5f})\")\n",
    "\n",
    "# Fit and predict\n",
    "stack.fit(X, y)\n",
    "preds = np.expm1(stack.predict(X_test))\n",
    "\n",
    "# Triple-blend with previous submissions (0.5 new + 0.3 prev_1 + 0.2 prev_2)\n",
    "blended_preds = 0.5 * preds + 0.3 * prev_submission_1['SalePrice'].values + 0.2 * prev_submission_2['SalePrice'].values\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame({\"Id\": test_ID, \"SalePrice\": blended_preds})\n",
    "submission.to_csv(\"triple_submission.csv\", index=False)\n",
    "print(\"Triple-blended submission created! Submit to Kaggle to check your score.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
